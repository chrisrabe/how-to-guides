# splunkhelper v1.2.3
import json
import calendar, time
import requests
from splunk import Intersplunk
 
## ========= Helper Functions =========
 
# Checks if the text is JSON
def _load_json(text):
    try:
        json_object = json.loads(text)
    except ValueError:
        return None
    return json_object
 
# Parses text that are in stash format.
# stash is a data format used by splunk when storing events without a specified source type
# This is typically encountered when being passed splunk events which are generated by the "collect" command
# It starts with the event time, followed by information about the search, and then followed by the data you want
# Key-value pairs are separated by an equals symbol.
# Each pair is split with a comma
def _parse_stash_string(text):
    # We want to exclude search information
    blacklist_keys = [
        'search_now',
        'info_search_time',
        'search_name'
    ]
    clean_data = {}
    raw_value = text.split(',')
    # Step 1: Filter out unnecessary data
    last_key = None
    for value in raw_value:
        key_value = value.split('=')
        if len(key_value) == 2:
            last_key = key_value[0].strip()
            if last_key not in blacklist_keys:
                value = key_value[1]
                clean_data[last_key] = value
        elif last_key is not None:
            # This value belongs to the previous key but was split accidentally
            clean_data[last_key] = clean_data[last_key] + ', ' + value
 
    # Step 2: Remove double quotations at the start and end of concatenated strings
    # If the key is assigned to multiple words, it attaches double quotations at the start and end of the value
    for key in clean_data:
        clean_data[key] = clean_data[key].strip('\"')
     
    return clean_data
 
# Retrieves the _raw property from the splunk event
# If the raw object is JSON, it returns the JSON object
# Otherwise the _raw value uses a special parsing algorithm
# That converts 'stash' sourcetypes into a dictionary
def _get_raw_object(splunk_event):
    if '_raw' not in splunk_event:
        return splunk_event
    raw = splunk_event['_raw']
    json_object = _load_json(raw)
    if json_object is not None:
        return json_object
    else:
        return _parse_stash_string(raw)
 
# adds the _time property into each element in the data list
# the timestamp is in Epoch format
def _add_timestamp(data_list):
    for i in range(0, len(data_list)):
        data = data_list[i]
        if not hasattr(data, '_time'):
            data['_time'] = calendar.timegm(time.gmtime())
 
# ensures that all elements in the data list is a dictionary
# if it's not a dictionary (e.g. a string or a number), it creates a dictionary with a "data" property
def _convert_to_dict(data_list):
    for i in range(0, len(data_list)):
        data = data_list[i]
        if not isinstance(data, dict):
            data_list[i] = {
                'data': data
            }
 
# Ensures that the data is a dictionary so that it can be queried through splunk.
# This is used for the HEC output function.
def _create_hec_event(data):
    event = { 'event': None }
 
    if not isinstance(data, dict):
        event['event'] = {
            'value': data
        }
    else:
        event['event'] = data
     
    return event
 
## ========= Public Functions =========
 
# Retrieves objects with properties from the _raw field of the splunk events.
def get_objects(splunk_events, additional_fields = []):
    filter_results = []
    fields = None # fields from the _raw property
 
    for event in splunk_events:
        # Retrieve _raw properties once
        # The reason why we do not use the values from the _raw object is because
        # the values assigned to properties are all strings (e.g. numbers + objects).
        if fields is None:
            raw = _get_raw_object(event)
            fields = [field for field in raw] + additional_fields
        filter_data = {}
        for field in fields:
            if field in event:
                filter_data[field] = event[field]
 
        filter_results.append(filter_data)
 
    return filter_results
 
# Logs out the message into the splunk error message
def debug(message):
    Intersplunk.generateErrorResults(message)
 
# Handles events from splunk.
# The output of the filter_callback is the input of the run_callback
#
# The run_callback argument is a function that takes the output of the filter_callback as the argument
#
# The filter_callback argument specifies how the splunk events are filtered
# By default, it retrieves the _raw field from the event and places them into the array
def handle_events(run_callback, filter_callback = None):
    splunk_events = Intersplunk.readResults()
    # Filter the events
    events = []
    if filter_callback == None:
        events = get_objects(splunk_events)
    else:
        events = filter_callback(splunk_events)
 
    run_callback(events)
 
# Outputs the data into splunk STDOUT
# When you are using this, make sure that your command.conf looks like this when you're declaring your script:
#
# [helloworld] <- identifier of the script that's invoked
# overrides_timeorder = true  <- without this, splunk would complain that the timestamps need to be sorted.
# filename = helloworld.py <- the python file that's executed
#
# To push the output to an index, you have to use the 'collect' query in Splunk
# E.g. ...| script myscript | collect index="my_index"
# This pushes the output of the script into "my_index" index
def output(data):
    output_data = data
 
    # Step 1: Ensure that arguments is a list
    # when using outputResults, you have to pass in an array.
    # This step automagically converts whatever the "data" argument into an array
    if not isinstance(data, list):
        output_data = [data]
     
    # Step 2: Validate the data
    # Need to ensure that the data that comes into splunk is a dictionary so that you can query properties from it
    _convert_to_dict(output_data)
 
    # Step 3: Add timestamp
    # Splunk uses Epoch time format. Automagically add the _time property into each element in the array if it does not exist
    _add_timestamp(output_data)
 
    # Last step: Push data into splunk
    Intersplunk.outputResults(output_data)
 
# Send data to the Splunk HTTP Event Collector
# To use this function, you need the instance URL, a HEC Token, as well as the data being sent
# To create a HEC token, from Splunk, go to Settings>Data Inputs>HTTP Event Collector>New Token
# Depending on how you set up your HEC token, it is optional to specify the index that the event is being sent if you have a default index
# on your HEC token.
#
# If the splunk server has an nginx proxy in place, a custom endpoint (splunkHEC) can be used to send
# data to the HTTP event collector. If the splunk server do not have an nginx proxy, the HEC port must be used to send data to splunk.
#
def output_hec(splunk_url, hec_token, data, index = None, has_nginx = True):
 
    # attach HEC Endpoint URL
    hec_endpoint = '/splunkHEC/services/collector/event' if has_nginx else ':8088/services/collector/event'
    hec_endpoint = splunk_url + hec_endpoint
 
    # Create event
    event = _create_hec_event(data)
 
    # Specify index if it exists
    if index is not None:
        event['index'] = index
 
    # Create request
    req_args = {
        'headers': {
            'Authorization': 'Splunk '+ hec_token,
            'content-type': 'application/json;charset=UTF-8'
        },
        'verify': False,
        'data': json.dumps(event)
    }
 
    # Send request
    response = requests.post(hec_endpoint, **req_args)
    return response